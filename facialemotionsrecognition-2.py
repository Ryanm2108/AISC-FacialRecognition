# -*- coding: utf-8 -*-
"""FacialEmotionsRecognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h6v-YY8jx8W9eCi5sH7tu73yjGtHDxdc
"""

# Image Loading and Preprocessing

import kagglehub

# Download latest version
path = kagglehub.dataset_download("msambare/fer2013")

print("Path to dataset files:", path)

import os
import cv2
import glob
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

def load_images(directory):
  images = []
  labels = []
  for label, emotion in enumerate(os.listdir(directory)):
    emotion_path = os.path.join(directory, emotion)
    if not os.path.isdir(emotion_path):
      continue # Skip if not a directory
    for filename in glob.glob(os.path.join(emotion_path, "*.jpg")):
      img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE) # Convert to grayscale
      img = cv2.resize(img, (48, 48))  # Resize
      img_vector = img.flatten() / 255.0 # Flatten + Normalize
      images.append(img_vector)
      labels.append(label)
  return np.array(images), np.array(labels)

# Define dataset directories
train_dir = "/root/.cache/kagglehub/datasets/msambare/fer2013/versions/1/train" #path to our train dataset
test_dir = "/root/.cache/kagglehub/datasets/msambare/fer2013/versions/1/test" #path to our test dataset

# Load images and labels
train_images, train_labels = load_images(train_dir)
test_images, test_labels = load_images(test_dir)


# DATA SPLITTING

train_images, val_images, train_labels, val_labels = train_test_split(
    train_images,
    train_labels,
    test_size=0.2,
    stratify = train_labels,
    random_state=42
)

# train_images, train_labels - are your training data and labels
# val_images, val_labels - are your validation data and labels
# test_images, test_labels - are your test data and labels

# Standardization
scaler = StandardScaler()
train_images= scaler.fit_transform(train_images)
val_images = scaler.transform(val_images)
test_images= scaler.transform(test_images)

# Exploratory Data Analysis (EDA)

import matplotlib.pyplot as plt

class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

def display_images(images, labels, num_images=5):
  fig, axes = plt.subplots(1, num_images, figsize=(12, 3))
  for i in range(num_images):
    idx = np.random.randint(0, len(images))
    img = images[idx].reshape(48, 48) # Reshape back to 2D
    axes[i].imshow(img, cmap='gray')
    axes[i].set_title(class_names[labels[idx]])
    axes[i].axis('off')
  plt.show()

display_images(train_images, train_labels)

# Class Distribution Bar Chart
unique, counts = np.unique(train_labels, return_counts=True)
plt.bar(unique, counts)
plt.xticks(unique, class_names)
plt.title('Class Distribution')
plt.show()

# Pixel Histogram
plt.hist(train_images.flatten(), bins=50, color='blue', alpha=0.7)
plt.title('Pixel Value Distribution')
plt.xlabel('Pixel Intensity')
plt.ylabel('Frequency')
plt.show()

# Saving the files
np.save("train_images.npy", train_images)
np.save("train_labels.npy", train_labels)
np.save("val_images.npy", val_images)
np.save("val_labels.npy", val_labels)
np.save("test_images.npy", test_images)
np.save("test_labels.npy", test_labels)

import numpy as np

# Load the data
X_train = np.load("train_images.npy")
y_train = np.load("train_labels.npy")
X_val = np.load("val_images.npy")
y_val = np.load("val_labels.npy")
X_test = np.load("test_images.npy")
y_test = np.load("test_labels.npy")

import numpy as np
import pandas as pd
import os
import cv2
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from scipy.stats import randint
import joblib
import kagglehub
from google.colab import drive, files  # <--- ADDED: Import drive and files

# --- 1. Mount Google Drive (EVERY TIME you start a new session) ---
drive.mount('/content/drive')

# --- 2. Define model_path (WHERE to save in Drive) ---
#    IMPORTANT:  CHANGE THIS PATH TO YOUR DESIRED FOLDER!
drive_path = '/content/drive/MyDrive/emotion_models'  #  <-- CHANGE THIS!
model_filename = 'best_decision_tree_model.pkl'
model_path = os.path.join(drive_path, model_filename)

# Create the directory in Drive if it doesn't exist. VERY IMPORTANT
os.makedirs(drive_path, exist_ok=True)

# --- 3. Download and Prepare Data (using kagglehub) ---
path = kagglehub.dataset_download("msambare/fer2013")
print("Path to dataset files:", path)

train_dir = os.path.join(path, "train")
test_dir = os.path.join(path, "test")

def load_images(directory):
    images = []
    labels = []
    for label, emotion in enumerate(os.listdir(directory)):
        emotion_path = os.path.join(directory, emotion)
        if not os.path.isdir(emotion_path):
            continue
        for filename in glob.glob(os.path.join(emotion_path, "*.jpg")):
            img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
            if img is None:
                print(f"Warning: Could not read image {filename}")
                continue
            img = cv2.resize(img, (48, 48))
            img_vector = img.flatten() / 255.0
            images.append(img_vector)
            labels.append(label)
    return np.array(images), np.array(labels)

train_images, train_labels = load_images(train_dir)
test_images, test_labels = load_images(test_dir)

scaler = StandardScaler()
train_images = scaler.fit_transform(train_images)
test_images = scaler.transform(test_images)

X_train, X_val, y_train, y_val = train_test_split(
    train_images, train_labels, test_size=0.2, stratify=train_labels, random_state=42
)

# --- 4. Train and Evaluate Baseline Decision Tree ---
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
y_val_pred = dt_model.predict(X_val)
print(f"Decision Tree (Baseline) Accuracy: {accuracy_score(y_val, y_val_pred)}")
print(classification_report(y_val, y_val_pred))
print(confusion_matrix(y_val, y_val_pred))

# --- 5. Hyperparameter Tuning ---
param_distributions = {
    'max_depth': randint(2, 15),
    'min_samples_leaf': randint(1, 10),
    'criterion': ['gini', 'entropy']
}

randomized_search = RandomizedSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=20,
    cv=3,
    scoring='accuracy',
    verbose=2,
    n_jobs=-1,
    random_state=42
)

randomized_search.fit(X_train, y_train)
print("Best Decision Tree Parameters:", randomized_search.best_params_)
best_dt_model = randomized_search.best_estimator_
print(f"Best Decision Tree Cross-Validated Accuracy: {randomized_search.best_score_}")

val_accuracy = best_dt_model.score(X_val, y_val)
print(f"Best Decision Tree Accuracy on Validation Set: {val_accuracy}")
print(classification_report(y_val, best_dt_model.predict(X_val)))
print(confusion_matrix(y_val, best_dt_model.predict(X_val)))

test_accuracy = best_dt_model.score(X_test, y_test)
print(f"Best Decision Tree Accuracy on Test Set: {test_accuracy}")
print(classification_report(y_test, best_dt_model.predict(X_test)))
print(confusion_matrix(y_test, best_dt_model.predict(X_test)))

# --- 6. Save to Google Drive AND Download ---
try:
    joblib.dump(best_dt_model, model_path)
    print(f"Model saved to Google Drive: {model_path}")
except Exception as e:
    print(f"ERROR saving to Google Drive: {e}")
    # import sys  # Uncomment if you want to exit on save failure
    # sys.exit(1)

try:
    files.download(model_path)
    print("Model downloaded to local computer.")
except Exception as e:
    print(f"ERROR downloading model: {e}")
    print("You should still have a copy in your Google Drive.")

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Define the best parameters manually
best_dt_model = DecisionTreeClassifier(
    max_depth=8,
    min_samples_leaf=4,
    criterion='entropy',
    random_state=42
)

# 2. Train the model on training data
best_dt_model.fit(X_train, y_train)

# 3. Make predictions on the validation set
y_val_pred_best_dt = best_dt_model.predict(X_val)

# 4. Evaluate the model
accuracy = accuracy_score(y_val, y_val_pred_best_dt)
print(f"Best Decision Tree Accuracy on Validation Set: {accuracy}")

print("Classification Report:\n", classification_report(y_val, y_val_pred_best_dt))
print("Confusion Matrix:\n", confusion_matrix(y_val, y_val_pred_best_dt))

# 5. Evaluate on the test set
y_test_pred_best_dt = best_dt_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred_best_dt)
print(f"Best Decision Tree Accuracy on Test Set: {test_accuracy}")

# random forest

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
y_val_pred_rf = rf_model.predict(X_val)
accuracy = accuracy_score(y_val, y_val_pred_rf)
print(f"Random Forest Accuracy: {accuracy}")
		#proportion of correctly classified examples
print(classification_report(y_val, y_val_pred_rf))
print(confusion_matrix(y_val, y_val_pred_rf))
		#true positive, tn, fp, fn

#Get precision (TP/(TP + FP)), recall (TP/(TP + FN)
precision = precision_score(y_val, y_val_pred_rf)
recall = recall_score(y_val, y_val_pred_rf)
#F1-score (2*(Precision*Recall)/(Precision+Recall))
f1 = f1_score(y_val, y_val_pred_rf)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-Score: {f1}")

# Hyperparameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [4, 8, 12],
    'min_samples_leaf': [2, 4, 6],
    'criterion': ['gini', 'entropy']
}

# GridSearchCV for hyperparameter tuning
grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)

# Best parameters found
print('Best parameters: ', grid.best_params_)


# Set hyperparameters
'''
rf_model = RandomForestClassifier(
    n_estimators=100,      # Number of trees in the forest
    max_depth=10,          # Maximum depth of each tree
    min_samples_leaf=2,    # Minimum samples required in a leaf node
    random_state=42
)
'''

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Define the best parameters manually
best_rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_leaf=2,
    random_state=42
)

# 2. Train the model on training data
best_rf_model.fit(X_train, y_train)

# 3. Make predictions on the validation set
y_val_pred_best_rf = best_rf_model.predict(X_val)

# 4. Evaluate the model
accuracy = accuracy_score(y_val, y_val_pred_best_rf)
print(f"Best Random Forest Accuracy on Validation Set: {accuracy}")

print("Classification Report:\n", classification_report(y_val, y_val_pred_best_rf))
print("Confusion Matrix:\n", confusion_matrix(y_val, y_val_pred_best_rf))

# 5. Evaluate on the test set
y_test_pred_best_rf = best_rf_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred_best_rf)
print(f"Best Random Forest Accuracy on Test Set: {test_accuracy}")

# logistic regression

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(random_state=42, max_iter=1000)
# Increase max_iter if needed^
lr_model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score

y_val_pred_lr = lr_model.predict(X_val)
accuracy = accuracy_score(y_val, y_val_pred_lr)
print(f"Logistic Regression Accuracy: {accuracy}")
		#proportion of correctly classified examples
print(classification_report(y_val, y_val_pred_lr))
print(confusion_matrix(y_val, y_val_pred_lr))
		#true positive, tn, fp, fn

#Get precision (TP/(TP + FP)), recall (TP/(TP + FN)
precision = precision_score(y_val, y_val_pred_lr, average='weighted')
recall = recall_score(y_val, y_val_pred_lr, average='weighted')
#F1-score (2*(Precision*Recall)/(Precision+Recall))
f1 = f1_score(y_val, y_val_pred_lr, average='weighted')

print(f"Precision (Weighted): {precision}")
print(f"Recall (Weighted): {recall}")
print(f"F1-Score (Weighted): {f1}")

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distributions = {
    'C': [0.01, 0.1, 1, 10, 100],            # Regularization strength
    'penalty': ['l1', 'l2', 'elasticnet'],   # Type of regularization
    'solver': ['liblinear', 'saga'],         # Solvers supporting penalties
    'max_iter': [100, 200, 500]              # Iterations for convergence
}

randomized_search = RandomizedSearchCV(estimator=lr_model, param_distributions=param_distributions, n_iter=10, cv=2, scoring='accuracy', verbose=2, n_jobs=-1)
randomized_search.fit(X_train, y_train)

# Best parameters found
print("Best Parameters:", randomized_search.best_params_)

#KNN

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score

knn_model = KNeighborsClassifier()
y_val = knn_model.predict(X_val)
accuracy = accuracy_score(y_val, y_val_pred_knn)
print(f"K Nearest Neighbors Accuracy: {accuracy}")
		#proportion of correctly classified examples
print(classification_report(y_val, y_val_pred_knn))
print(confusion_matrix(y_val, y_val_pred_knn))
		#true positive, tn, fp, fn

#Get precision (TP/(TP + FP)), recall (TP/(TP + FN)
precision = precision_score(y_val, y_val_pred_knn, average='weighted')
recall = recall_score(y_val, y_val_pred_knn, average='weighted')
#F1-score (2*(Precision*Recall)/(Precision+Recall))
f1 = f1_score(y_val, y_val_pred_knn, average='weighted')

print(f"Precision (Weighted): {precision}")
print(f"Recall (Weighted): {recall}")
print(f"F1-Score (Weighted): {f1}")

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Hyperparameter distribution
param_distributions = {
    'n_neighbors': [3, 5, 7, 10],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

randomized_search = RandomizedSearchCV(estimator=knn_model, param_distributions=param_distributions, n_iter=10, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)
randomized_search.fit(X_train, y_train)

# Best parameters found
print("Best Parameters:", randomized_search.best_params_)


# Set hyperparameters
'''
knn_model = KNeighborsClassifier(
    n_neighbors=7,
    metric='manhattan'
)
'''

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Define the best parameters manually
best_knn_model = KNeighborsClassifier(
    n_neighbors=7,
    metric='manhattan'
)

# 2. Train the model on training data
best_knn_model.fit(X_train, y_train)

# 3. Make predictions on the validation set
y_val_pred_best_knn = best_knn_model.predict(X_val)

# 4. Evaluate the model
accuracy = accuracy_score(y_val, y_val_pred_best_knn)
print(f"Best KNN Accuracy on Validation Set: {accuracy}")

print("Classification Report:\n", classification_report(y_val, y_val_pred_best_knn))
print("Confusion Matrix:\n", confusion_matrix(y_val, y_val_pred_best_knn))

# 5. Evaluate on the test set
y_test_pred_best_knn = best_knn_model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred_best_knn)
print(f"Best KNN Accuracy on Test Set: {test_accuracy}")

# decision tree accuracy on test set: 0.3110894399554193
# random forest accuracy on test set: 0.40596266369462247
# logistic regression accuracy on test set:
# knn accuracy on test set:

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np

y_test = test_labels

# 2. Select the BEST model after hyperparameter tuning
best_model = best_rf_model

# 3. Make predictions on the test set
y_test_pred = best_model.predict(X_test)

# 4. Evaluate performance
print("Test Set Accuracy:", accuracy_score(y_test, y_test_pred))
print(classification_report(y_test, y_test_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))